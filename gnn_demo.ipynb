{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35b4809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fefc7c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_93650/158744771.py:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(GRAPH_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded graph with shape: x=torch.Size([19032, 504]), edge_index=torch.Size([2, 29514972])\n",
      "🔢 Encoded classes: ['Tier 1', 'Tier 2', 'Tier 3A', 'Tier 3B']\n",
      "🎯 Number of classes: 4\n",
      "📊 Train: 3017, Val: 431, Test: 863\n",
      "Epoch 010 | Loss: 1.3264 | Train: 0.257 | Val: 0.278 | Test: 0.260\n",
      "Epoch 020 | Loss: 1.3158 | Train: 0.337 | Val: 0.348 | Test: 0.335\n",
      "Epoch 030 | Loss: 1.3141 | Train: 0.358 | Val: 0.390 | Test: 0.355\n",
      "Epoch 040 | Loss: 1.3047 | Train: 0.423 | Val: 0.425 | Test: 0.401\n",
      "Epoch 050 | Loss: 1.2992 | Train: 0.371 | Val: 0.381 | Test: 0.357\n",
      "Epoch 060 | Loss: 1.2971 | Train: 0.389 | Val: 0.392 | Test: 0.359\n",
      "Epoch 070 | Loss: 1.2880 | Train: 0.291 | Val: 0.304 | Test: 0.263\n",
      "Epoch 080 | Loss: 1.2814 | Train: 0.349 | Val: 0.385 | Test: 0.348\n",
      "Epoch 090 | Loss: 1.2772 | Train: 0.381 | Val: 0.392 | Test: 0.377\n",
      "📤 Output logits (first 5):\n",
      "tensor([[-0.5885, -0.7711, -0.2612, -0.1269],\n",
      "        [-0.3180, -0.2921, -0.3181, -0.1037],\n",
      "        [-0.1031, -0.6414, -0.1241, -0.4198],\n",
      "        [-0.7087, -0.7578, -0.2281,  0.0040],\n",
      "        [-0.4229, -0.4754, -0.0475,  0.0746]], device='cuda:0')\n",
      "🎯 Predicted classes (first 20): [3, 3, 0, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 1, 0, 3, 1, 3, 1]\n",
      "🧾 Ground truth labels (first 20): [3, -1, 3, 2, -1, -1, -1, -1, -1, -1, 3, -1, -1, 1, -1, 0, -1, -1, -1, -1]\n",
      "📊 Class distribution of predictions:\n",
      "  Class 0: 1315 nodes\n",
      "  Class 1: 641 nodes\n",
      "  Class 2: 790 nodes\n",
      "  Class 3: 1565 nodes\n",
      "Epoch 100 | Loss: 1.2782 | Train: 0.426 | Val: 0.408 | Test: 0.404\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# ---------- 设置 ----------\n",
    "GRAPH_PATH = \"/home/zihend1/Genesis/KNOT/data/gene_graph.pt\"\n",
    "LABEL_TYPE = \"druggability_tier\"\n",
    "NUM_EPOCHS = 100\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda:0\" if USE_GPU else \"cpu\")\n",
    "\n",
    "# ---------- 模型定义 ----------\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# ---------- 加载图 ----------\n",
    "data = torch.load(GRAPH_PATH)\n",
    "print(f\"✅ Loaded graph with shape: x={data.x.shape}, edge_index={data.edge_index.shape}\")\n",
    "\n",
    "# ---------- 清理特征 ----------\n",
    "nan_mask = torch.isnan(data.x)\n",
    "if nan_mask.any():\n",
    "    print(f\"⚠️ NaNs in features: {nan_mask.sum().item()}\")\n",
    "    data.x[nan_mask] = 0\n",
    "\n",
    "# ---------- 标签编码 ----------\n",
    "if not isinstance(data.y, torch.Tensor):\n",
    "    y_np = np.array(data.y)\n",
    "else:\n",
    "    y_np = data.y.cpu().numpy()\n",
    "\n",
    "# 如果是字符串标签\n",
    "if y_np.dtype.type is np.str_ or y_np.dtype.type is np.object_:\n",
    "    known_mask = y_np != \"-1\"\n",
    "    le = LabelEncoder()\n",
    "    le.fit(y_np[known_mask])\n",
    "    y_encoded = np.full_like(y_np, fill_value=-1, dtype=int)\n",
    "    y_encoded[known_mask] = le.transform(y_np[known_mask])\n",
    "    data.y = torch.tensor(y_encoded, dtype=torch.long)\n",
    "    print(f\"🔢 Encoded classes: {list(le.classes_)}\")\n",
    "else:\n",
    "    data.y = torch.tensor(y_np, dtype=torch.long)\n",
    "    print(\"✅ Labels already encoded as integers\")\n",
    "\n",
    "num_classes = int(data.y[data.y != -1].max().item()) + 1\n",
    "print(f\"🎯 Number of classes: {num_classes}\")\n",
    "\n",
    "# ---------- 数据划分 ----------\n",
    "def split_masks(y, val_ratio=0.1, test_ratio=0.2, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    idx = np.arange(len(y))\n",
    "    known = idx[y != -1]\n",
    "    y_known = y[known].numpy()\n",
    "\n",
    "    train_idx, temp_idx = train_test_split(known, test_size=val_ratio + test_ratio, stratify=y_known)\n",
    "    val_idx, test_idx = train_test_split(temp_idx, test_size=test_ratio / (val_ratio + test_ratio), stratify=y[temp_idx].numpy())\n",
    "\n",
    "    mask = lambda ids: torch.tensor([i in ids for i in range(len(y))])\n",
    "    return mask(train_idx), mask(val_idx), mask(test_idx)\n",
    "\n",
    "data.train_mask, data.val_mask, data.test_mask = split_masks(data.y)\n",
    "print(f\"📊 Train: {data.train_mask.sum().item()}, Val: {data.val_mask.sum().item()}, Test: {data.test_mask.sum().item()}\")\n",
    "\n",
    "# ---------- 类别权重 ----------\n",
    "train_labels = data.y[data.train_mask].cpu().numpy()\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float, device=DEVICE)\n",
    "\n",
    "# ---------- 模型初始化 ----------\n",
    "model = GCN(in_channels=data.x.shape[1], hidden_channels=64, out_channels=num_classes).to(DEVICE)\n",
    "data = data.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "# ---------- 训练 ----------\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask], weight=class_weights)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(print_details=False):\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    pred = out.argmax(dim=1)\n",
    "\n",
    "    if print_details:\n",
    "        print(f\"📤 Output logits (first 5):\\n{out[:5]}\")\n",
    "        print(f\"🎯 Predicted classes (first 20): {pred[:20].tolist()}\")\n",
    "        print(f\"🧾 Ground truth labels (first 20): {data.y[:20].tolist()}\")\n",
    "        pred_counts = Counter(pred[data.y != -1].tolist())\n",
    "        print(\"📊 Class distribution of predictions:\")\n",
    "        for c in range(num_classes):\n",
    "            print(f\"  Class {c}: {pred_counts.get(c, 0)} nodes\")\n",
    "\n",
    "    results = {}\n",
    "    for split in ['train_mask', 'val_mask', 'test_mask']:\n",
    "        mask = getattr(data, split)\n",
    "        correct = (pred[mask] == data.y[mask]).sum().item()\n",
    "        acc = correct / mask.sum().item()\n",
    "        results[split] = acc\n",
    "    return results\n",
    "\n",
    "# ---------- 主循环 ----------\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    loss = train()\n",
    "    results = evaluate(print_details=(epoch == NUM_EPOCHS))\n",
    "    if epoch % 10 == 0 or epoch == NUM_EPOCHS:\n",
    "        print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | Train: {results['train_mask']:.3f} | Val: {results['val_mask']:.3f} | Test: {results['test_mask']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6988935d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4698f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e0d433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808b645a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
